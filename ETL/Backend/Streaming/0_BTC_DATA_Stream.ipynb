{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Already exists\n",
      "No new data to be added, try going to Yahoo Finance to updated data form today!\n",
      "('klines', 'BTCBUSD-1m-2023-04-03.zip') doesnt exist yet \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fn/0t1gngls6x39txrdzml_kv3w0000gn/T/ipykernel_1608/3820750002.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col].loc[row] = '.'.join(df[col].loc[row].split('.')[:2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed klines\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import os\n",
    "from zipfile import BadZipfile\n",
    "\n",
    "\n",
    "\n",
    "def download_url(args):\n",
    "    t0 = time.time()\n",
    "    url, fn = args[0], args[1]\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        with open(fn, 'wb') as f: \n",
    "            f.write(r.content)\n",
    "        return(url, time.time() - t0)\n",
    "    except Exception as e:\n",
    "        print('Exception in download_url():', e)\n",
    "\n",
    "\n",
    "# Upload most recent full csv\n",
    "\n",
    "def get_num_days_since_update(init=False):\n",
    "    from datetime import datetime\n",
    "\n",
    "    if init:\n",
    "        last_entry_dt = \"2021-01-12 8:00:00\"\n",
    "    else:\n",
    "        df = pd.read_csv('./../../Database/Futures_um/klines/Full_Data_klines.csv')\n",
    "        last_entry_dt = (df['open_time'].iloc[-1])\n",
    "\n",
    "    last_entry_datetime = datetime.strptime(last_entry_dt, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    import datetime\n",
    "    base = datetime.datetime.today()\n",
    "    num_days_since_last_entry = str(base -  last_entry_datetime).split(' ')[0]\n",
    "    return num_days_since_last_entry\n",
    "\n",
    "def get_date_list(last_entry_dt,all_dates = False):\n",
    "    if all_dates:\n",
    "        last_entry_dt = \"2021-01-12 8:00:00\"\n",
    "    else:\n",
    "        df = pd.read_csv('./../../Database/Futures_um/klines/Full_Data_klines.csv')\n",
    "        last_entry_dt = (df['open_time'].iloc[-1])\n",
    "    import datetime\n",
    "    base = datetime.datetime.today()\n",
    "\n",
    "    from datetime import datetime\n",
    "    last_entry_datetime = datetime.strptime(last_entry_dt, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    num_days_since_last_entry = str(base -  last_entry_datetime).split(' ')[0]\n",
    "\n",
    "    import datetime\n",
    "    date_list = [base - datetime.timedelta(days=x) for x in range(int(num_days_since_last_entry)+1)]\n",
    "    return date_list\n",
    "# Create functons to create the database structure\n",
    "\n",
    "def create_Fodler_Structure():\n",
    "    if len(os.listdir(\"./../../Database\")) < 2 :\n",
    "        for k in ['Futures']:    \n",
    "            for i in ['um']:\n",
    "                for j in ['klines']:\n",
    "                    os.makedirs(f\"./../../Database/{k}_{i}/{j}\")\n",
    "    else:\n",
    "        print(f'Folder Already exists')\n",
    "\n",
    "# Create funciton to gather urls from page and locations, and put them in a list\n",
    "\n",
    "# Create funciton to gather urls from page and locations, and put them in a list\n",
    "\n",
    "def get_urls_and_locations(init=False,parent_dir=['um'] , typeOfData=['klines']):\n",
    "    import datetime\n",
    "    if init:\n",
    "        num_days_since_last_entry = get_num_days_since_update(init=True) # Seems correct!\n",
    "\n",
    "    else:\n",
    "        num_days_since_last_entry = get_num_days_since_update(init=False) # Seems correct!\n",
    "\n",
    "    if len(num_days_since_last_entry) < 15 :\n",
    "        \n",
    "        date_list = [datetime.datetime.today() - datetime.timedelta(days=x) for x in range(int(num_days_since_last_entry)+1)]\n",
    "        \n",
    "        urls_dict={'klines':[],'aggTrades':[],'trades':[],'indexKLines':[],'MarkPriceKLines':[],'premiumIndexKLines':[],'trendingMetrics':[]}\n",
    "        fns_dict = {'klines':[],'aggTerades':[],'trades':[],'indexKLines':[],'MarkPriceKLines':[],'premiumIndexKLines':[],'trendingMetrics':[]}\n",
    "\n",
    "        parent_dir = parent_dir\n",
    "        \n",
    "        for i in range(len(date_list)):\n",
    "            try:\n",
    "                for j in parent_dir:\n",
    "                    for k in typeOfData:\n",
    "                        date = str(date_list[i])[:10]\n",
    "                        urlk = f\"https://data.binance.vision/data/futures/{j}/daily/klines/BTCBUSD/1m/BTCBUSD-1m-\"+date+\".zip\"\n",
    "\n",
    "\n",
    "                        lock = f\"./../../Database/Futures_{j}/{k}/BTCBUSD-1m-\"+date+\".zip\"\n",
    "                        \n",
    "\n",
    "                        urls_dict[k].append(urlk)\n",
    "                    \n",
    "\n",
    "                        fns_dict[k].append(lock)\n",
    "            except Exception as e:\n",
    "                print(e,date_list[i])\n",
    "                break\n",
    "        return date_list , urls_dict , fns_dict\n",
    "    else:\n",
    "        print(\"No new data to be added, try going to Yahoo Finance to updated data form today!\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Downlaod all the data from the links\n",
    "# Data form BTC daily, for klines , delays about ~10 minutes form Jan 2021 to 2023\n",
    "\n",
    "def download_data_binance(urls_dict,fns_dict, typeOfData=['klines']):\n",
    "    t0 = time.time()\n",
    "\n",
    "    for j in typeOfData:  #['klines','aggTrades','trades','indexKLines','MarkPriceKLines','premiumIndexKLines','trendingMetrics']:\n",
    "        try: \n",
    "            inputs = zip(urls_dict[j], fns_dict[j])\n",
    "            for i in inputs:\n",
    "                result = download_url(i)\n",
    "                print('url:', result[0], 'time:', result[1])\n",
    "            print('Total time:', time.time() - t0)\n",
    "        except TypeError:\n",
    "            continue\n",
    "\n",
    "# MERGE\n",
    "\n",
    "# Check if the first row is as column header and if so, fix it and add headers\n",
    "\n",
    "def perform_sanity_checks(typeOfData=['klines']):\n",
    "    for directory in typeOfData:\n",
    "        li = list(os.listdir(f'./../../Database/Futures_um/klines'))\n",
    "        li.sort()      \n",
    "        if '.DS_Store' in li:\n",
    "            li.remove('.DS_Store')\n",
    "        if 'Full_Data_klines.csv' in li:\n",
    "            li.remove('Full_Data_klines.csv')\n",
    "\n",
    "        for elem in li:\n",
    "            try:\n",
    "                # momprint(directory , elem)\n",
    "                df = pd.read_csv(f'./../../Database/Futures_um/{directory}/{elem}')\n",
    "                fr = pd.DataFrame(df.columns.to_list()).T\n",
    "                if fr.columns.tolist() == ['open_time'\t, 'open'\t, 'high', \t'low'\t, 'close',\t'volume'\t,'close_time'\t,'quote_volume'\t,'count',\t'taker_buy_volume'\t,'taker_buy_quote_volume'\t,'ignore']:\n",
    "                    print(f\"{elem} doesnt change\")\n",
    "                else:\n",
    "                    fr.columns = ['open_time'\t, 'open'\t, 'high', \t'low'\t, 'close',\t'volume'\t,'close_time'\t,'quote_volume'\t,'count',\t'taker_buy_volume'\t,'taker_buy_quote_volume'\t,'ignore']\n",
    "                    df.columns = ['open_time'\t, 'open'\t, 'high', \t'low'\t, 'close',\t'volume'\t,'close_time'\t,'quote_volume'\t,'count',\t'taker_buy_volume'\t,'taker_buy_quote_volume'\t,'ignore']\n",
    "                    df_conc = pd.concat([fr,df])\n",
    "                    df_conc = df_conc.loc[(df_conc['open_time'] != 'open_time')]\n",
    "                    df_conc.to_csv(f'./../../Database/Futures_um/{directory}/{elem}',index=False,compression='zip')\n",
    "            except BadZipfile:\n",
    "                print(f\"{directory , elem} doesnt exist yet \")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e , elem)\n",
    "                continue\n",
    "\n",
    "# Create quickly num Sanity Check\n",
    "\n",
    "def floats_sanity_check(df):\n",
    "    for col in ['high', 'low', 'open', 'close', 'taker_buy_volume', 'taker_buy_quote_volume']:\n",
    "        for row in range(df.shape[0]):\n",
    "            try:\n",
    "                if len(df[col].loc[row].split('.')) > 2:\n",
    "                    df[col].loc[row] = '.'.join(df[col].loc[row].split('.')[:2])\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return df\n",
    "\n",
    "def merge_into_single_csv(date_list,init=False,debug=False,typeOfData=['klines']):    \n",
    "    # do a mega csv of each of te folder components\n",
    "    for directory in typeOfData:\n",
    "        try:\n",
    "            if init:\n",
    "                base_df = pd.DataFrame(columns=['open_time'\t, 'open'\t, 'high', \t'low'\t, 'close',\t'volume'\t,'close_time'\t,'quote_volume'\t,'count',\t'taker_buy_volume'\t,'taker_buy_quote_volume'\t,'ignore'])\n",
    "            elif debug :\n",
    "                base_df = pd.read_csv('./../../Database/Futures_um/klines/BTCBUSD-1m-2021-01-12.zip')\n",
    "\n",
    "            else:\n",
    "                base_df = pd.read_csv('./../../Database/Futures_um/klines/Full_Data_klines.csv')\n",
    "\n",
    "            for i in range(len(date_list)):\n",
    "                dat = str(date_list[i])[:10]\n",
    "                link = f'./../../Database/Futures_um/{directory}/BTCBUSD-1m-{dat}.zip'\n",
    "                #print(dat)\n",
    "                try :\n",
    "                    df = pd.read_csv(link)\n",
    "                    base_df = pd.concat([base_df , df])\n",
    "                except BadZipfile:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(e , date_list[i])\n",
    "                    continue\n",
    "                \n",
    "            base_df.reset_index(drop=True,inplace=True)\n",
    "            base_df['open_time'] = base_df['open_time'].apply(lambda x: x/1)\n",
    "\n",
    "            #base_df['open_time']=pd.to_datetime(base_df['open_time'],unit='ns')\n",
    "            base_df['open_time']=pd.to_datetime(base_df['open_time'],unit='ms')\n",
    "\n",
    "            base_df.sort_values('open_time',inplace=True)\n",
    "            base_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "            merged_df = floats_sanity_check(base_df)\n",
    "\n",
    "\n",
    "            merged_df.to_csv(f'./../../Database/Futures_um/{directory}/Full_Data_klines.csv',index=False)\n",
    "            print(f'completed klines')\n",
    "        except Exception as e:\n",
    "            print(e,dat)\n",
    "            continue\n",
    "    \n",
    "        #return merged_df\n",
    "\n",
    "def create_Stream(init=False):\n",
    "    if init==True:\n",
    "        \n",
    "        date_list , urls_dict , fns_dict = get_urls_and_locations(init=init)\n",
    "        download_data_binance(urls_dict , fns_dict)\n",
    "        perform_sanity_checks()\n",
    "        merge_into_single_csv(date_list)\n",
    "\n",
    "    else:\n",
    "        create_Fodler_Structure()\n",
    "        try:\n",
    "            date_list , urls_dict , fns_dict = get_urls_and_locations(init=init)\n",
    "            download_data_binance(urls_dict , fns_dict)\n",
    "            perform_sanity_checks()\n",
    "            merge_into_single_csv(date_list)\n",
    "        except TypeError:\n",
    "            \n",
    "            last_entry_dt = \"2021-01-12 8:00:00\"\n",
    "            from datetime import datetime\n",
    "            last_entry_datetime = datetime.strptime(last_entry_dt, '%Y-%m-%d %H:%M:%S')\n",
    "            import datetime\n",
    "            num_days_since_last_entry = str(datetime.datetime.today() -  last_entry_datetime).split(' ')[0]\n",
    "            date_list = [datetime.datetime.today() - datetime.timedelta(days=x) for x in range(int(num_days_since_last_entry)+1)]\n",
    "\n",
    "            perform_sanity_checks()\n",
    "            merge_into_single_csv(date_list,debug=True)\n",
    "\n",
    "\n",
    "create_Stream(init=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'. BTCBUSD-1m-2021-12-07.zip\n",
      "('klines', 'BTCBUSD-1m-2023-04-04.zip') doesnt exist yet \n"
     ]
    }
   ],
   "source": [
    "last_entry_dt = \"2021-01-12 8:00:00\"\n",
    "from datetime import datetime\n",
    "last_entry_datetime = datetime.strptime(last_entry_dt, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "import datetime\n",
    "num_days_since_last_entry = str(datetime.datetime.today() -  last_entry_datetime).split(' ')[0]\n",
    "date_list = [datetime.datetime.today() - datetime.timedelta(days=x) for x in range(int(num_days_since_last_entry)+1)]\n",
    "\n",
    "perform_sanity_checks()\n",
    "merge_into_single_csv(date_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(date_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
